{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TauFakeFactors","text":"<p>FakeFactor framework for the estimation of jets misidentified taus with pyROOT.</p>"},{"location":"#setup","title":"Setup","text":"<p>Clone the repository via</p> <pre><code>git clone --recurse-submodules https://github.com/KIT-CMS/TauFakeFactors.git\n</code></pre> <p>The environment can be set up with conda via</p> <pre><code>conda env create --file environment.yml\n</code></pre>"},{"location":"#general-configuration","title":"General configuration","text":"<p>General definitions like paths for all steps of the fake factor measurements should be defined in a <code>configs/ANALYSIS/ERA/common_settings.yaml</code> file (this file name should always stay the same).</p> <p>The expected ntuple folder structure is NTUPLE_PATH/ERA/SAMPLE_TAG/CHANNEL/*.root</p> parameter type description <code>ntuple_path</code> <code>string</code> absolute path to the folder with the n-tuples on the dcache, a remote path is expected like \"root://cmsdcache-kit-disk.gridka.de//store/user/USER/...\" <code>friends</code> <code>list</code> (optional) list of friend names that exist for the n-tuples in <code>ntuple_path</code> <code>tree</code> <code>string</code> name of the tree in the n-tuple files (\"ntuple\" in CROWN) <code>era</code> <code>string</code> data taking era (e.g. \"2018, \"2017\", \"2016preVFP\", \"2016postVFP\") <code>nanoAOD_version</code> <code>string</code> definition of the nanoAOD version is relevant to calculate the correct generator weights for the later specified simulated sample as well as getting the correct cross sections <code>tau_vs_jet_wps</code> <code>list</code> list of tau ID vsJet working points to be written out in the preselection step (e.g. [\"Medium\", \"VVVLoose\"]) <code>tau_vs_jet_wgt_wps</code> <code>list</code> list of tau ID vsJet working point scale factors to be written out in the preselection step (e.g. [\"Medium\"]) <p>The output folder structure is OUTPUT_PATH/preselection/ERA/CHANNEL/*.root</p> parameter type description <code>output_path</code> <code>string</code> absolute path where the files with the preselected events will be stored, a local path is expected like \"/ceph/USER/...\" <code>file_path</code> <code>string</code> absolute path to the folder with the preselected files (should be the same as <code>output_path</code>) to be used for the fake factor calculation <code>workdir_name</code> <code>string</code> relative path where the fake factor measurement output files will be stored; folder is produced in <code>workdir/</code> <p>The parameters in <code>common_settings.yaml</code> will be overwritten if they are also specified in the config for the individual steps of the fake factor determination, like event preselection, fake factor calculation and correction calculation.  </p>"},{"location":"#hints","title":"Hints","text":"<ul> <li>check out <code>configs/general_definitions.py</code>, this file has many relevant definition for plotting (dictionaries for names) and correctionlib output information</li> <li>check <code>ntuple_path</code> and <code>output_path</code> (preselection) and <code>file_path</code> and <code>workdir_name</code> (fake factors, corrections) in the used config files to avoid wrong inputs or outputs</li> </ul>"},{"location":"binning/","title":"Automated Equipopulated Binning","text":"<p>The framework includes a utility script, <code>adjust_binning.py</code>, for an automatic calculation and update binning definitions within the configuration in case of equipopulated binning approach of specified variable (<code>var_dependence</code>) across various categories for a more robust statistical treatment in each analysis region. The general intention is to have an equipopulated data distribution in the SRlike region given possible additional splittings (i.e. in <code>njets</code>).</p> <p>The script is intended to be run berofe the fake factor and correction calculation by using</p> <pre><code>python adjust_binning.py --config PATH/CONFIG.yaml --cut-config PATH/CONFIG_WITH_REGION_CUTS.yaml --processes QCD Wjets --cut-region ARlike\n</code></pre> Argument Type Description <code>--config</code> <code>string</code> Path to the YAML configuration file that will be modified. <code>--cuts-config</code> <code>string</code> (Optional) Path to a separate YAML file containing cut definitions. This is useful for correction YAML files where cuts are not specified directly for each correction but then are sourced from the cuts-config file, which can just be the fake factor YAML configuration. <code>--processes</code> <code>list[string]</code> A list of processes to adjust (e.g., <code>QCD</code>, <code>Wjets</code>, <code>ttbar</code>, <code>process_fractions</code>). The default is set to include all processes. <code>--cut-region</code> <code>string</code> The region cut to be applied when determining event counts (e.g., <code>SRlike</code>, <code>ARlike</code>). <code>process_fractions</code> always use the <code>AR_cut</code>. <code>--dry-run</code> <code>bool</code> Preview the binning calculations and changes without modifying the config file. <code>--dataset</code> <code>string</code> Name of the dataset used for equipopulated binning adjustments. Default is set to 'data', deviations can be made for i.e. MC studies."},{"location":"binning/#configuration","title":"Configuration","text":"<p>To enable this feature, you must add a <code>equipopulated_binning_options</code> block to the relevant process or correction section in your configuration file. The script uses this block to generate the new binning and will populate the <code>var_bins</code> key and update <code>split_categories</code> and <code>split_categories_binedges</code> if they are left blank i.e. for continuous variables such as <code>pt_1</code>.</p> <p>For example, a manually binned configuration for the <code>QCD</code> process:</p> <pre><code># Before: Manual binning for QCD process\ntarget_processes:\n  QCD:\n    split_categories:\n      njets: [\"==0\", \"==1\", \"&gt;=2\"]\n    split_categories_binedges:\n      njets: [-0.5, 0.5, 1.5, 12.5]\n    var_dependence: pt_2\n    var_bins: \n      \"==0\": [30.0, 31.6, 33.5, 35.9, 38.9, 43.6, 51.6, 150.0]\n      \"==1\": [30.0, 32.2, 35.3, 40.3, 50.9, 150.0]\n      \"&gt;=2\": [30.0, 32.7, 36.5, 42.5, 57.0, 150.0]\n</code></pre> <p>Can be replaced with a configuration for automated binning:</p> <pre><code># After: Configuration for automated binning\ntarget_processes:\n  QCD:\n    split_categories:\n      njets: [\"==0\", \"==1\", \"&gt;=2\"]\n    split_categories_binedges:\n      njets: [-0.5, 0.5, 1.5, 12.5]\n    var_dependence: pt_2\n    var_bins: {} # This will be filled by the script\n    equipopulated_binning_options:\n      variable_config:\n        pt_2:\n          min: 30.0\n          max: 150.0\n          rounding: 2\n      n_bins:  # this part is necessarily only needed for continuous variables\n        njets: 3  # number of bins for njets ==0, ==1, &gt;=2\n      var_dependence_n_bins: [7, 5, 5]\n</code></pre> <p>The <code>equipopulated_binning_options</code> block has the following structure:</p> Parameter Type Description <code>variable_config</code> <code>dict</code> Defines parameters for variables used in binning. See sub-parameters below. <code>n_bins</code> <code>dict</code> Specifies the desired number of bins per variable in <code>split_categories</code> if those are not set there in case of continuous variables, i.e. four equidistant bins in <code>pt_1</code>: <code>pt_1: 4</code> <code>var_dependence_n_bins</code> <code>int</code> or <code>list[int]</code> or <code>list[list[int]]</code> Number of bins used for the variable defined in <code>var_dependence</code>, either as int (used by all categories) or a (nested) list of integers defining number of bins per (nested) category created. <p>The <code>variable_config</code> for each variable contains:</p> Sub-parameter Type Description <code>min</code> <code>float</code> The lower bound for the variable's range. The first bin edge will be set to this value. This is applied as a cut before the calculation of equipopulated binning starts. <code>max</code> <code>float</code> The upper bound for the variable's range. The last bin edge will be set to this value. This is applied as a cut before the calculation of equipopulated binning starts. <code>rounding</code> <code>int</code> The number of decimal places to which the calculated bin edges will be rounded to and written into the configuration file."},{"location":"binning/#advanced-splitting-strategies","title":"Advanced Splitting Strategies","text":"<p>The script supports various one- and two-dimensional splitting strategies, determined by the structure of <code>split_categories</code>. The order of variables defines the nesting hierarchy.</p> <ol> <li>One-Dimensional Splitting</li> </ol> <ul> <li> By a discrete variable (e.g., <code>njets</code>): Categories are explicitly listed.  Number of bins of `var_dependence` in each category are set explicitly  <pre><code>split_categories:\n  njets: [\"==0\", \"==1\", \"&gt;=2\"]\nsplit_categories_binedges:\n  njets: [-0.5, 0.5, 1.5, 12.5]\nequipopulated_binning_options:\n  variable_config:\n    pt_2:\n      min: 30.0\n      max: 150.0\n      rounding: 4\n  var_dependence_n_bins: [7, 5, 5]\n</code></pre>  Same number of bins of `var_dependence` in each category  <pre><code>split_categories:\n  njets: [\"==0\", \"==1\", \"&gt;=2\"]\nsplit_categories_binedges:\n  njets: [-0.5, 0.5, 1.5, 12.5]\nequipopulated_binning_options:\n  variable_config:\n    pt_2:\n      min: 30.0\n      max: 150.0\n      rounding: 4\n  var_dependence_n_bins: 7\n</code></pre> </li> <li> By a continuous variable (e.g., <code>pt_1</code>): An empty list <code>[]</code> is used as a placeholder.   The script will first create equipopulated bins for `pt_1` and then bin `var_dependence` within each of those new `pt_1` categories.   <pre><code>split_categories:\n  pt_1: []\nsplit_categories_binedges:\n  pt_1: []\nequipopulated_binning_options:\n  variable_config:\n    pt_2: \n      min: 30.0\n      max: 150.0\n      rounding: 4\n    pt_1: \n      min: 0.0\n      max: 1000.0\n      rounding: 2\n  n_bins:\n    pt_1: 4 # Creates 4 categories for pt_1, which are then used for pt_2 binning\n  var_dependence_n_bins: [7, 7, 6, 5] # number of bins per pt_1 split \n</code></pre> </li> </ul> <ol> <li>Two-Dimensional (Nested) Splitting</li> </ol> <ul> <li> Two discrete variables (e.g., <code>tau_decaymode_2</code> then <code>njets</code>).   You can also merge categories using the <code>#||#</code> syntax.   <pre><code>split_categories:\n  tau_decaymode_2: [\"==0\", \"==1\", \"==10\", \"==11\"]\n  njets: [\"==0\", \"==1\", \"&gt;=2\"]\nsplit_categories_binedges:\n  tau_decaymode_2: [-0.5, 0.5, 9.5, 11.5]\n  njets: [-0.5, 0.5, 1.5, 12.5]\nequipopulated_binning_options:\n  variable_config:\n    pt_2: \n      min: 30.0\n      max: 150.0\n      rounding: 4\n  var_dependence_n_bins: [[9, 8, 7], [7, 7, 7], 6, 6]  # here 6 == [6, 6, 6] \n\n</code></pre> </li> <li> Discrete then continuous variable (e.g., <code>njets</code> then <code>pt_1</code>).   `pt_1` is binned equipopulated within each `njets` category.   <pre><code>split_categories:\n  njets: [\"==0\", \"==1\", \"&gt;=2\"]\n  pt_1: [] # Placeholder for equipopulated split\nsplit_categories_binedges:\n  njets: [-0.5, 0.5, 1.5, 12.5]\n  pt_1: [] # Placeholder for equipopulated split\nequipopulated_binning_options:\n  variable_config:\n    pt_2: \n      min: 30.0\n      max: 150.0\n      rounding: 4\n    pt_1: \n      min: 0.0\n      max: 3000.0\n      rounding: 2\n  n_bins:\n    pt_1: 4   # Create 4 pt_1 bins inside each njets bin\n  var_dependence_n_bins: [[9, 8, 7, 7], 10, 6]  # logic analogously to Two discrete variables example\n</code></pre> </li> <li> Two continuous variables (e.g., <code>deltaR_ditaupair</code> then <code>pt_1</code>). <pre><code>split_categories:\n  deltaR_ditaupair: []\n  pt_1: []\nsplit_categories_binedges:\n  deltaR_ditaupair: []\n  pt_1: []\nequipopulated_binning_options:\n  variable_config:\n    pt_2: \n        min: 30.0\n        max: 150.0\n        rounding: 4\n    deltaR_ditaupair:\n      min: 0.0\n      max: 5.0\n      rounding: 3\n    pt_1:\n      min: 0.0\n      max: 3000.0\n      rounding: 2\n  n_bins:\n    deltaR_ditaupair: 2 # Create 2 bins for the first split\n    pt_1: 4             # Create 4 bins for the second split\n  var_dependence_n_bins: [[10, 10, 5, 5], [8, 8, 4, 4]]\n</code></pre> </li> </ul>"},{"location":"corrections/","title":"Fake Factor corrections","text":"<p>In this step the corrections for the fake factors are calculated. This should be run after the FF calculation step.</p> <p>Currently two different correction types are implemented: </p> <ol> <li>non closure correction depending on a specific variable</li> <li>DR to SR interpolation correction depending on a specific variable</li> </ol> <p>All information for the FF correction calculation step is defined in a configuration file in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> and a more specific config file. Additional information is loaded from the used config in the previous FF calculation step (this is done automatically).  The FF correction config has the following parameters:</p> <p>The expected input folder structure is workdir/WORKDIR_NAME/ERA/fake_factors/CHANNEL/*</p> parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <p>In <code>target_processes</code> the processes for which FF corrections should be calculated (normally for QCD, Wjets, ttbar) are defined.</p> <p><code>split_categories</code> can be set for <code>non_closure</code> and <code>DR_SR</code> corrections (1D only) accordingly. <code>var_bins</code> declaration follow the specifications named in <code>FakeFactor calculation</code> section  Each target process needs some specifications:</p> parameter type description <code>non_closure</code> <code>dict</code> one or two non closure corrections can be specified indicated by the variable the correction should be calculated for (e.g. <code>pt_1</code>), also more than one closure correction are allowed and are calculated while already applying the correction that were already measured <code>DR_SR</code> <code>dict</code> this correction should be specified only once per process in <code>target_processes</code> <code>chain_DR_SR_to_non_closure</code> <code>bool</code> Option to chain <code>DR_SR</code> correction (computed first) to the following <code>non_closure</code> corrections <p>Each correction has following specifications:</p> parameter type description <code>var_dependence</code> <code>string</code> variable the FF correction measurement should depend on (e.g. <code>\"pt_1\"</code>) <code>split_categories</code> <code>dict</code> Optional, analogous to <code>FakeFactor calculation</code> (only 1D). <code>var_bins</code> <code>list</code> or <code>dict[list]</code> Analogous to <code>var_bins</code> in <code>FakeFactor calculation</code> <code>correction_option</code> <code>str</code> <code>correction_option</code> <code>bandwidth</code> <code>float</code> if <code>correction_option</code> includes <code>\"smoothed\"</code> this value can be set to adjust for the bandwidth used during smoothing procedure (has no effect on the result in case of <code>\"binwise\"</code>). If not set the default value of histogram range divided by 5 is chosen.  Can either be a float value representing the bandwidth for all corrections or a dictionary of float values, where keys correspond to the string representations of split categories defined in <code>split_categories</code>. <code>SRlike_cuts</code> <code>dict</code> event selections for the signal-like region of the target process that should be replaced compared to the selection used in the previous FF calculation step <code>ARlike_cuts</code> <code>dict</code> event selections for the application-like region of the target process that should be replaced compared to the selection used in the previous FF calculation step <code>AR_SR_cuts</code> <code>dict</code> event selections for a switch from the determination region to the signal/application region, this is only relevant for <code>DR_SR</code> corrections <code>non_closure</code> <code>dict</code> this is only relevant for <code>DR_SR</code> corrections, since for this corrections additional fake factors are calculated. It's possible to calculated and apply non closure corrections to these fake factors before calculating the actual DR to SR correction. <p>To run the FF correction step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_corrections.py --config-file PATH/CONFIG.yaml \n</code></pre> <p>There are two optional parameters <code>--skip-DRtoSR-ffs</code> and <code>--only-main-corrections</code>. The correction caclulation is done in 3 steps.  The first step is to calculate additional fake factors which are needed for the final DR to SR correction. If this is already done, this step can be skipped using <code>--skip-DRtoSR-ffs</code>.  The second step is to calculate non closure corrections for these additional DR to SR fake factors. If both steps are already done they can be skipped by using <code>--only-main-corrections</code>.  The last step is to calculate all the specified corrections for the main fake factors.</p>"},{"location":"documentation/","title":"Create Documentation","text":"<p>The fake factor environment has <code>mkdocs</code> already installed. The  documentation is build from the <code>*.md</code> files in the <code>docs/</code>  folder based on the config file <code>mkdocs.yml</code>.</p> <p>The documentation page can be produced locally with:</p> <pre><code>mkdocs build\n</code></pre> <p>This results in a new folder <code>site/</code> where you can have a look at  how the documentation is rendered. To see the result, open  <code>site/index.html</code> in the browser of your choice.</p>"},{"location":"fakefactors/","title":"Fake Factor calculation","text":"<p>In this step the fake factors are calculated. This should be run after the preselection step.</p> <p>All information for the FF calculation step is defined in a configuration file in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> and a more specific config file.  The FF calculation config has the following parameters:</p> <p>General options for the calculation:</p> parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <code>use_embedding</code> <code>bool</code> True if embedded sample should be used, False if only MC sample should be used <code>use_center_of_mass_bins</code> <code>bool</code> Changes the x-data that is entering FF and correction calculation. If set then a center of mass value is used for the x-data, calculated from events entering the corresponding bin. If not set, the bin centers are used. Default is set to True.  This will not affect FF and correction calculation that are set to <code>\"binwise\"</code> (the x-data values although displayed in plots are not used) <p>In <code>target_processes</code> the processes for which FFs should be calculated (normally for QCD, Wjets, ttbar) are defined.  Each target process needs some specifications:</p> parameter type description <code>split_categories</code> <code>dict</code> names of variables for the fake factor measurement in different phase space regions <ul><li>the FF measurement can be split based on variables in 1D or 2D (1 or 2 variables)</li><li>each category/variable has a <code>list</code> of orthogonal cuts (e.g. \"njets\" with \"==1\", \"&gt;=2\")</li><li> \"njets\", \"nbtag\", \"tau_decaymode_2\" or \"deltaR_ditaupair\" are already possible, other variables should be added during preprocessing step accordingly </li><li>at least one inclusive category needs to be specified (assuming variable is written out in preselection step)</li></ul> If a continous variable is used a window can be defined as <code>\"&gt;=lower#&amp;&amp;#&lt;upper\"</code> accordingly. <code>split_categories_binedges</code> <code>dict</code> bin edge values for each <code>split_categories</code> variable.  The number of bin edges should always be N(variable cuts)+1 <code>SRlike_cuts</code> <code>dict</code> event selections for the signal-like region of the target process <code>ARlike_cuts</code> <code>dict</code> event selections for the application-like region of the target process <code>SR_cuts</code> <code>dict</code> event selections for the signal region (normally only needed for ttbar) <code>AR_cuts</code> <code>dict</code> event selections for the application region (normally only needed for ttbar) <code>var_dependence</code> <code>string</code> variable the FF measurement should depend on (normally pt of the hadronic tau e.g. <code>\"pt_2\"</code>) <code>var_bins</code> <code>list</code> or <code>dict[list]</code> bin edges for the variable specified in <code>var_dependence</code>.  Can either be a list representing the binning or a dictionary of lists, where keys correspond to the string representations of split categories defined in <code>split_categories</code>.  In the case of two split categories, the dictionary can be nested. If not all second split category elements share the first split category binning, the binning for the affected category must be specified separately. When using split binning, at least the first split category's bin edges must be fully defined. <code>fit_options</code> <code>list</code> a list of polynomials that should be considered for the fake factor fits can be defined with this parameter (default: <code>[\"poly_1\"]</code>); futher it is possible to specify <code>\"binwise\"</code> which means that the histograms are written out directly without a fit <code>limit_kwargs</code> <code>dict</code> this dictionary allows to define how the fitted function and its uncertainty are handled (also outside the measurement range); the default is that outside the range the fake factor functions stays constant but the up and down variations still increase/decrease; additionally negative fake factor values are not allowed and if present are set to 0 <p>Event selections can be defined the same way as in the preselection step <code>event_selection</code>. Only the tau vs jet ID cut is special because the name should always be <code>had_tau_id_vs_jet</code> (or <code>had_tau_id_vs_jet_*</code> in tt channel), this is needed to read out the working points from the cut string and apply the correct tau vs jet ID weights.</p> <p>In <code>process_fractions</code> specifications for the calculation of the process fractions are defined.</p> parameter type description <code>processes</code> <code>list</code> sample names (from the preprocessing step) of the processes for which the fractions should be stored in the correctionlib json, the sum of fractions of the specified samples is 1. <code>split_categories</code> <code>dict</code> see <code>target_processes</code> (only in 1D) <code>AR_cuts</code> <code>list</code> see <code>target_processes</code> <code>SR_cuts</code> <code>list</code> see <code>target_processes</code>, (optional) not needed for the fraction calculation <p>Note: When using split binning for process fraction calculations, the <code>var_bins</code> parameter can also be defined in the same manner as for <code>target_processes</code>.</p> <p>To run the FF calculation step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_calculation.py --config-file PATH/CONFIG.yaml\n</code></pre>"},{"location":"preselection/","title":"Event preselection","text":"<p>This framework is designed for n-tuples (and friend trees) produced with CROWN as input.  All information for the preselection step is defined in configuration files in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> file and a more specific config file. </p> <p>The preselection config has the following parameters:</p> parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <code>processes</code> <code>dict</code> process parameters are explained below <code>event_selection</code> <code>dict</code> with this parameter all selections that should be applied are defined. This is basically a dictionary of cuts where the key is the name of a cut and the value is the cut itself as a string e.g. <code>had_tau_pt: \"pt_2 &gt; 30\"</code>. The name of a cut is not really important, it is only used as an output information in the terminal. A cut can only use variables which are in the ntuples. <code>mc_weights</code> <code>dict</code> weight parameter are defined below <code>emb_weights</code> <code>dict</code> all weights that should be applied for embedded samples are defined. Like for <code>event_selection</code> a weight can directly be specified and is then applied to all samples the same way e.g. <code>single_trigger: \"trg_wgt_single_mu24ormu27\"</code> <code>output_features</code> <code>list</code> the to be saved/needed features for the later calculations are listed, this also includes features from friend trees if <code>friends</code> were specified in <code>common_settings.yaml</code> <p>In <code>processes</code> all the processes are defined that should be preprocessed.  The names are also used for the output file naming after the processing.  Each process needs two specifications:</p> subparameter type description <code>tau_gen_modes</code> <code>list</code> split of the events corresponding to the origin of the hadronic tau <code>samples</code> <code>list</code> list of all sample tags corresponding to the specific process <p>The <code>tau_gen_modes</code> have following modes:</p> subparameter type description <code>T</code> <code>string</code> genuine tau <code>J</code> <code>string</code> jet misidentified as a tau <code>L</code> <code>string</code> lepton misidentified as a tau <code>all</code> <code>string</code> if no split should be performed <p>In <code>mc_weights</code> all weights that should be applied for simulated samples are defined.  There are two types of weights.</p> <ol> <li>Similar to <code>event_selection</code>, a weight can directly be specified and is then applied to all samples in the same way e.g. <code>lep_id: \"id_wgt_mu_1\"</code></li> <li> <p>But some weights are either sample specific or need additional information. Currently implemented options are:</p> subparameter type description <code>generator</code> <code>string</code> The normal generator weight is applied to all samples, if they aren't specified in the <code>\"stitching\"</code> sub-group. Stitching weights might be needed for DY+jets or W+jets, depending on which samples are used for them. <code>lumi</code> <code>string</code> luminosity scaling, this depends on the era and uses the <code>era</code> parameter of the config to get the correct weight, so basically it's not relevant what is in the string <code>Z_pt_reweight</code> <code>string</code> reweighting of the Z boson pt, the weight in the ntuple is used and only applied to DY+jets <code>Top_pt_reweight</code> <code>string</code> reweighting of the top quark pt, the weight in the ntuple is used and only applied to ttbar </li> </ol> <p>Scale factors for b-tagging and tau ID vs jet are applied on the fly during the FF calculation step. </p> <p>To run the preselection step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python preselection.py --config-file configs/PATH/CONFIG.yaml\n</code></pre> <p>Further there are additional optional parameters: </p> <ol> <li><code>--nthreads=SOME_INTEGER</code> to define the number of threads for the multiprocessing pool to run the sample processing in parallel. Default value is 8 (this should normally cover running all of the samples in parallel).</li> <li><code>--ncores=SOME_INTEGER</code> to define the number of cores that should be used for each pool thread to speed up the ROOT dataframe calculation. Default value is 2.</li> </ol>"}]}