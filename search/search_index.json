{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TauFakeFactors","text":"<p>FakeFactor framework for the estimation of jets misidentified taus with pyROOT.</p>"},{"location":"#setup","title":"Setup","text":"<p>Clone the repository via</p> <pre><code>git clone --recurse-submodules https://github.com/KIT-CMS/TauFakeFactors.git\n</code></pre> <p>The environment can be set up with conda via</p> <pre><code>conda env create --file environment.yaml\n</code></pre> <p>General definitions like paths for all steps of the fake factor measurements should be defined in a <code>configs/ANALYSIS/ERA/common_settings.yaml</code> file (file name should be always stay the same).</p> <p>The expected ntuple folder structure is NTUPLE_PATH/ERA/SAMPLE_TAG/CHANNEL/*.root</p> parameter type description <code>ntuple_path</code> <code>string</code> absolute path to the folder with the n-tuples on the dcache, a remote path is expected like \"root://cmsxrootd-kit.gridka.de//store/user/USER/...\" <code>tree</code> <code>string</code> name of the tree in the n-tuple files (\"ntuple\" in CROWN) <code>era</code> <code>string</code> data taking era (e.g. \"2018, \"2017\", \"2016preVFP\", \"2016postVFP\") <code>tau_vs_jet_wps</code> <code>list</code> list of tau ID vsJet working points to be written out in the preselection step (e.g. [\"Medium\", \"VVVLoose\"]) <code>tau_vs_jet_wgt_wps</code> <code>list</code> list of tau ID vsJet working point scale factors to be written out in the preselection step (e.g. [\"Medium\"]) <p>The output folder structure is OUTPUT_PATH/preselection/ERA/CHANNEL/*.root</p> parameter type description <code>output_path</code> <code>string</code> absolute path where the files with the preselected events will be stored, a local path is expected like \"/ceph/USER/...\" <code>file_path</code> <code>string</code> absolute path to the folder with the preselected files (should be the same as <code>output_path</code>) to be used for the fake factor calculation <code>workdir_name</code> <code>string</code> relative path where the fake factor measurement output files will be stored; folder is produced in <code>workdir/</code>"},{"location":"#event-preselection","title":"Event preselection","text":"<p>This framework is designed for n-tuples produced with CROWN as input.  All information for the preselection step is defined in configuration files in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> file and a more specific config file. </p> <p>The preselection config has the following parameters:</p> *   parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <ul> <li> <p>In <code>processes</code> all the processes are defined that should be preprocessed. \\   The names are also used for the output file naming after the processing. \\   Each process needs two specifications:</p> subparameter type description <code>tau_gen_modes</code> <code>list</code> split of the events corresponding to the origin of the hadronic tau <code>samples</code> <code>list</code> list of all sample tags corresponding to the specific process </li> </ul> <p>The <code>tau_gen_modes</code> have following modes:</p> <pre><code>subparameter | type | description\n---|---|---\n`T` | `string` | genuine tau\n`J` | `string` | jet misidentified as a tau\n`L` | `string` | lepton misidentified as a tau\n`all` | `string` | if no split should be performed\n</code></pre> <ul> <li> <p>In <code>event_selection</code>, parameter for all selections that should be applied are defined. \\   This is basically a dictionary of cuts where the key is the name of a cut and the value is the cut itself as a string e.g. <code>had_tau_pt: \"pt_2 &gt; 30\"</code>. The name of a cut is not really important, it is only used as an output information in the terminal. A cut can only use variables which are in the ntuples.</p> </li> <li> <p>In <code>mc_weights</code> all weights that should be applied for simulated samples are defined. \\   There are two types of weights.</p> </li> <li>Similar to <code>event_selection</code>, a weight can directly be specified and is then applied to all samples in the same way e.g. <code>lep_id: \"id_wgt_mu_1\"</code></li> <li>But some weights are either sample specific or need additional information. Currently implemented options are:</li> </ul> subparameter type description <code>generator</code> <code>string</code> The normal generator weight is applied to all samples, if they aren't specified in the <code>\"stitching\"</code> sub-group. Stitching weights might be needed for DY+jets or W+jets, depending on which samples are used for them. <code>lumi</code> <code>string</code> luminosity scaling, this depends on the era and uses the <code>era</code> parameter of the config to get the correct weight, so basically it's not relevant what is in the string <code>Z_pt_reweight</code> <code>string</code> reweighting of the Z boson pt, the weight in the ntuple is used and only applied to DY+jets <code>Top_pt_reweight</code> <code>string</code> reweighting of the top quark pt, the weight in the ntuple is used and only applied to ttbar <ul> <li>In <code>emb_weights</code> all weights that should be applied for embedded samples are defined. \\   Like for <code>event_selection</code> a weight can directly be specified and is then applied to all samples the same way e.g. <code>single_trigger: \"trg_wgt_single_mu24ormu27\"</code></li> <li>In <code>output_features</code> the to be saved/needed features for the later calculations are listed.</li> </ul> <p>Scale factors for b-tagging and tau ID vs jet are applied on the fly during the FF calculation step. </p> <p>To run the preselection step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python preselection.py --config-file configs/PATH/CONFIG.yaml\n</code></pre> <p>Further there are additional optional parameters:  1. <code>--nthreads=SOME_INTEGER</code> to define the number of threads for the multiprocessing pool to run the sample processing in parallel. Default value is 8 (this should normally cover running all of the samples in parallel). 2.  <code>--ncores=SOME_INTEGER</code> to define the number of cores that should be used for each pool thread to speed up the ROOT dataframe calculation. Default value is 4.</p>"},{"location":"#fake-factor-calculation","title":"Fake Factor calculation","text":"<p>In this step the fake factors are calculated. This should be run after the preselection step.</p> <p>All information for the FF calculation step is defined in a configuration file in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> and a more specific config file. \\ The FF calculation config has the following parameters:</p> <ul> <li>General options for the calculation:</li> </ul> parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <code>use_embedding</code> <code>bool</code> True if embedded sample should be used, False if only MC sample should be used <code>use_center_of_mass_bins</code> <code>bool</code> Changes the x-data that is entering FF and correction calculation. If set then a center of mass value is used for the x-data, calculated from events entering the corresponding bin. If not set, the bin centers are used. Default is set to True.  This will not affect FF and correction calculation that are set to <code>\"binwise\"</code> (the x-data values although displayed in plots are not used) <ul> <li>In <code>target_processes</code> the processes for which FFs should be calculated (normally for QCD, Wjets, ttbar) are defined. \\   Each target process needs some specifications:</li> </ul> parameter type description <code>split_categories</code> <code>dict</code> names of variables for the fake factor measurement in different phase space regions <ul><li>the FF measurement can be split based on variables in 1D or 2D (1 or 2 variables)</li><li>each category/variable has a <code>list</code> of orthogonal cuts (e.g. \"njets\" with \"==1\", \"&gt;=2\")</li><li> \"njets\", \"nbtag\", \"tau_decaymode_2\" or \"deltaR_ditaupair\" are already possible, other variables should be added during preprocessing step accordingly </li><li>at least one inclusive category needs to be specified (assuming variable is written out in preselection step)</li></ul> If a continous variable is used a window can be defined as <code>\"&gt;=lower#&amp;&amp;#&lt;upper\"</code> accordingly. <code>split_categories_binedges</code> <code>dict</code> bin edge values for each <code>split_categories</code> variable <ul><li>number of bin edges should always be N(variable cuts)+1rections <code>non_closure</code> <code>dict</code> this is only relevant for <code>DR_SR</code> corrections, since for this corrections additional fake factors are calculated. It's possible to calculated and apply non closure corrections to these fake factors before calculating the actual DR to SR correction. <p>To run the FF correction step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_corrections.py --config-file PATH/CONFIG.yaml \n</code></pre> <p>There are two optional parameters <code>--skip-DRtoSR-ffs</code> and <code>--only-main-corrections</code>. The correction caclulation is done in 3 steps.\\ The first step is to calculate additional fake factors which are needed for the final DR to SR correction. If this is already done, this step can be skipped using <code>--skip-DRtoSR-ffs</code>.\\ The second step is to calculate non closure corrections for these additional DR to SR fake factors. If both steps are already done they can be skipped by using <code>--only-main-corrections</code>.\\ The last step is to calculate all the specified corrections for the main fake factors. ons   <code>non_closure</code> | <code>dict</code> | this is only relevant for <code>DR_SR</code> corrections, since for this corrections additional fake factors are calculated. It's possible to calculated and apply non closure corrections to these fake factors before calculating the actual DR to SR correction.</p> <p>To run the FF correction step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_corrections.py --config-file PATH/CONFIG.yaml \n</code></pre> <p>There are two optional parameters <code>--skip-DRtoSR-ffs</code> and <code>--only-main-corrections</code>. The correction caclulation is done in 3 steps.\\ The first step is to calculate additional fake factors which are needed for the final DR to SR correction. If this is already done, this step can be skipped using <code>--skip-DRtoSR-ffs</code>.\\ The second step is to calculate non closure corrections for these additional DR to SR fake factors. If both steps are already done they can be skipped by using <code>--only-main-corrections</code>.\\ The last step is to calculate all the specified corrections for the main fake factors.  <code>SRlike_cuts</code> | <code>dict</code> | event selections for the signal-like region of the target process   <code>ARlike_cuts</code> | <code>dict</code> | event selections for the application-like region of the target process   <code>SR_cuts</code> | <code>dict</code> | event selections for the signal region (normally only needed for ttbar)   <code>AR_cuts</code> | <code>dict</code> | event selections for the application region (normally only needed for ttbar)   <code>var_dependence</code> | <code>string</code> | variable the FF measurement should depend on (normally pt of the hadronic tau e.g. <code>\"pt_2\"</code>)   <code>var_bins</code> | <code>list</code> or <code>dict[list]</code> | bin edges for the variable specified in <code>var_dependence</code>.  Can either be a list representing the binning or a dictionary of lists, where keys correspond to the string representations of split categories defined in <code>split_categories</code>.  In the case of two split categories, the dictionary can be nested. If not all second split category elements share the first split category binning, the binning for the affected category must be specified separately. When using split binning, at least the first split category's bin edges must be fully defined.   <code>fit_options</code> | <code>list</code> | a list of polynomials that should be considered for the fake factor fits can be defined with this parameter (default: <code>[\"poly_1\"]</code>); futher it is possible to specify <code>\"binwise\"</code> which means that the histograms are written out directly without a fit    <code>limit_kwargs</code> | <code>dict</code> | this dictionary allows to define how the fitted function and its uncertainty are handled (also outside the measurement range); the default is that outside the range the fake factor functions stays constant but the up and down variations still increase/decrease; additionally negative fake factor values are not allowed and if present are set to 0</p> <pre><code>Event selections can be defined the same way as in the preselection step `event_selection`. Only the tau vs jet ID cut is special because the name should always be `had_tau_id_vs_jet` (or `had_tau_id_vs_jet_*` in tt channel), this is needed to read out the working points from the cut string and apply the correct tau vs jet ID weights.\n</code></pre> <ul> <li>In <code>process_fractions</code> specifications for the calculation of the process fractions are defined.</li> </ul> parameter type description <code>processes</code> <code>list</code> sample names (from the preprocessing step) of the processes for which the fractions should be stored in the correctionlib json, the sum of fractions of the specified samples is 1. <code>split_categories</code> <code>dict</code> see <code>target_processes</code> (only in 1D) <code>AR_cuts</code> <code>list</code> see <code>target_processes</code> <code>SR_cuts</code> <code>list</code> see <code>target_processes</code>, (optional) not needed for the fraction calculation <pre><code>**Note:** When using split binning for process fraction calculations, the `var_bins` parameter can also be defined in the same manner as for `target_processes`.\n</code></pre> <p>To run the FF calculation step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_calculation.py --config-file PATH/CONFIG.yaml\n</code></pre>"},{"location":"#fake-factor-corrections","title":"Fake Factor corrections","text":"<p>In this step the corrections for the fake factors are calculated. This should be run after the FF calculation step.</p> <p>Currently two different correction types are implemented:  1. non closure correction depending on a specific variable 2. DR to SR interpolation correction depending on a specific variable</p> <p>All information for the FF correction calculation step is defined in a configuration file in the <code>configs/ANALYSIS/ERA/</code> folder using the <code>common_settings.yaml</code> and a more specific config file. Additional information is loaded from the used config in the previous FF calculation step (this is done automatically). \\ The FF correction config has the following parameters:</p> <ul> <li>The expected input folder structure is workdir/WORKDIR_NAME/ERA/fake_factors/CHANNEL/*</li> </ul> parameter type description <code>channel</code> <code>string</code> tau pair decay channels (\"et\", \"mt\", \"tt\") <ul> <li>In <code>target_processes</code> the processes for which FF corrections should be calculated (normally for QCD, Wjets, ttbar) are defined.</li> <li><code>split_categories</code> can be set for <code>non_closure</code> and <code>DR_SR</code> corrections (1D only) accordingly. <code>var_bins</code> declaration follow the specifications named in <code>FakeFactor calculation</code> section \\   Each target process needs some specifications:</li> </ul> parameter type description <code>non_closure</code> <code>dict</code> one or two non closure corrections can be specified indicated by the variable the correction should be calculated for (e.g. <code>pt_1</code>), also more than one closure correction are allowed and are calculated while already applying the correction that were already measured <code>DR_SR</code> <code>dict</code> this correction should be specified only once per process in <code>target_processes</code> <p>Each correction has following specifications:</p> parameter type description <code>var_dependence</code> <code>string</code> variable the FF correction measurement should depend on (e.g. <code>\"pt_1\"</code>) <code>split_categories</code> <code>dict</code> Optional, analogous to <code>FakeFactor calculation</code> (only 1D). <code>var_bins</code> <code>list</code> or <code>dict[list]</code> Analogous to <code>var_bins</code> in <code>FakeFactor calculation</code> <code>correction_option</code> <code>str</code> Definition of how a correction should be measured. Options are <code>\"smoothed\"</code> (applies a gaussian density kernel), <code>\"binwise\"</code> or both can also be combined into e.g. <code>\"binwise#[0,]+smoothed\"</code>, <code>\"binwise#[-1,-2,]+smoothed\"</code> or <code>\"binwise#[0,]#[-1,]+smoothed\"</code> where the nth-bin(s) is computed using binwise method and the rest in smoothed manner. For left bins, positive bin index is used. For last bins, the count is done using negative integers, starting at -1 (like in the example). If both options are provided, then the binwise calculation is applied for the stated left and right bins. <code>bandwidth</code> <code>float</code> if <code>correction_option</code> includes <code>\"smoothed\"</code> this value can be set to adjust for the bandwidth used during smoothing procedure (has no effect on the result in case of <code>\"binwise\"</code>). If not set the default value of histogram range divided by 5 is chosen.  Can either be a float value representing the bandwidth for all corrections or a dictionary of float values, where keys correspond to the string representations of split categories defined in <code>split_categories</code>. <code>SRlike_cuts</code> <code>dict</code> event selections for the signal-like region of the target process that should be replaced compared to the selection used in the previous FF calculation step <code>ARlike_cuts</code> <code>dict</code> event selections for the application-like region of the target process that should be replaced compared to the selection used in the previous FF calculation step <code>AR_SR_cuts</code> <code>dict</code> event selections for a switch from the determination region to the signal/application region, this is only relevant for <code>DR_SR</code> corrections <code>non_closure</code> <code>dict</code> this is only relevant for <code>DR_SR</code> corrections, since for this corrections additional fake factors are calculated. It's possible to calculated and apply non closure corrections to these fake factors before calculating the actual DR to SR correction. <p>To run the FF correction step, execute the python script and specify the config file (relative path possible):</p> <pre><code>python ff_corrections.py --config-file PATH/CONFIG.yaml \n</code></pre> <p>There are two optional parameters <code>--skip-DRtoSR-ffs</code> and <code>--only-main-corrections</code>. The correction caclulation is done in 3 steps.\\ The first step is to calculate additional fake factors which are needed for the final DR to SR correction. If this is already done, this step can be skipped using <code>--skip-DRtoSR-ffs</code>.\\ The second step is to calculate non closure corrections for these additional DR to SR fake factors. If both steps are already done they can be skipped by using <code>--only-main-corrections</code>.\\ The last step is to calculate all the specified corrections for the main fake factors. </p>"},{"location":"#hints","title":"Hints","text":"<ul> <li>check out <code>configs/general_definitions.py</code>, this file has many relevant definition for plotting (dictionaries for names) and correctionlib output information</li> <li>check <code>ntuple_path</code> and <code>output_path</code> (preselection) and <code>file_path</code> and <code>workdir_name</code> (fake factors, corrections) in the used config files to avoid wrong inputs or outputs</li> </ul>"}]}